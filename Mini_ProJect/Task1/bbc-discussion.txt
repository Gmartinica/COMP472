a) We believe the classes are balanced according to the pdf graph from part 2. Because we are working with multi-Class classification, we believe accuracy is a good metric for our data to make sure our algorithm classifies them correctly. The problem is when classes are very imbalanced that accuracy is not a good metric but, in our case, we are fine with accuracy.
b) There are of courses differences in the performance because of the different smoothing values used. By using smoothing values closer to 0, we are coming back to the zero-probability problem, where if we had a word that did not appear for a certain class in the training data, then the algorithm will assume the probability of categorizing the article as that class becomes closer and closer to zero. As a result, it is normally recommended to use Laplace smoothing with alpha = 1 to avoid misclassifications just because a word was not in the training data.